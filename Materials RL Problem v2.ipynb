{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ac9a22-555a-4ab5-bd88-886fc416c8a5",
   "metadata": {},
   "source": [
    "<h2>What’s the Problem? </h2>\n",
    "\n",
    "Semiconductor packages use a variety of insulating and conducting materials that are patterned in a layered structure. Those materials have different thermal conductivities and depending on their pattern within each layer, they will contribute differently to the overall effective thermal conductivity of the package. \n",
    "\n",
    "A customer is interested in choosing the optimal combination of materials per layer that maximizes the thermal conductivity in the “z” direction (shown in the figure below), and minimize the materials cost subject to constraints on the available choices of material. \n",
    "\n",
    "The Ask \n",
    "\n",
    "Train an RL agent from scratch using Ray-RLLib that chooses the “optimal” set of materials per layer on a semiconductor package layout given information about the cost per unit weight and available choices of materials per layer per example, and details about the spatial pattern of conducting and and insulating materials in each layer. \n",
    "\n",
    "What We’ll Look for\n",
    "\n",
    "- Resourcefulness - were you able to come up with a self-consistent toy environment on which to train your RL agent? How did you inform your environment design by pulling relevant domain expertise?\n",
    "- Creativity - this problem is intentionally pretty open but quite technical, which mirrors most problems at Vinci. Did you come up with a novel way of solving it? Where were you blocked and how did you get around those blockers?\n",
    "- Communication - How clearly can you summarize your approach, the solution, and its implicit and chosen shortcomings?\n",
    "\n",
    "Additional Details\n",
    "\n",
    "You will need to construct a custom RL environment that matches the problem statement. We don’t expect you to write an RL algorithm from scratch. Feel free to use out-of box algorithm(s) from RLLib You should think about how the optimal actions of the RL agent would be consumed downstream - what would serving this model in a product look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67e661b-5034-4089-84ca-c0503118a293",
   "metadata": {},
   "source": [
    "<h2> Approach </h2>\n",
    "- There are two approaches to using RL to handle this problem.\n",
    "\n",
    "- Single Step (Finite horizon of 1) - Each step of RL, the \"action\" is to perturb one (or potentially more) parameters, and run the evaluation to find the reward.  The reward is assumed to be determinitistic.  ie. each action is an operation over the entire stack.  Setting the *entire* configuration of all the layers is considered a single action.\n",
    "\n",
    "- Multistep (Horizon == Number of layers) - Each timestep, the action is setting the configuration of the next layer.  Layer 1 is set at t=1, layer 2 is set at t=2, given the configuration of layer 1, layer 3 is set at t=3 given the configurations of layers 1 and 2, etc...  Under this approach, reward can be calculated two ways:\n",
    "\n",
    "    - Delayed reward.  The reward is 0 at each layer, until after the final configuration is determined.  Then the overall reward is calculated.  This approach tends to converge slower as reward after the last step have to be propogated back to the decisions of the initial layers (ie.  How did the first layer have impact on the final outcome).  This is known as the credit assignment problem. This is analogous to using RL to solve a maze, but the only reward is given at the very end when the goal is reaching.  This then requires propogating the information back to the initial steps to figure out the best actions to take.\n",
    "  \n",
    "    - Incremental reward - Calculate a reward at each layer based on some heuristic, for example, looking at the cost and performance of the materials of the layer you just calculated.  The heuristic typically requires some domain knowledge, as you are essentially already suggesting a path.  (The heuristic ends up acting like an the value function).  This information can help lead to a result faster, but risks missing the best solution if the heuristic is poor, and potentially getting stuck in local minima.  Using the maze analogy, this would be applying a reward at each step based on the decrease in Manhattan distance to goal.  By encouraging actions which get closer to the goal, a solution might be found faster, but makes it difficult to find paths which have to move further away first but are better overall.  \n",
    "\n",
    "- RL is used here as an optimization.  The goal is to explore the space of configurations to find the best configuration given the objective function.  This is different than a typical training approach which searchs model parameter space to find the best set for performance over a large training set.\n",
    "\n",
    "- The exploration conducted via RL algorithms (trying random actions) will be used to explore the configuration space.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cf4617-0bd3-474d-9e4e-ca3b8be12f9e",
   "metadata": {},
   "source": [
    "For this implementation, we will use a multistep incremental reward implementation.  This can easily be modified to do the delayed reward approach.\n",
    "\n",
    "<h3>Action Space</h3>\n",
    "\n",
    "The action sapce is a 1-d array of integers where each position represents a configuration for a material.  For each step, only the values for the layers till the current layer are set (with the remainder of the array being clipped).  \n",
    "\n",
    "<h3>Observation Space</h3>\n",
    "\n",
    "The observation space includes information about the current layer which is about to be set.  Includes the thickness, the thermal material, and the insulating material. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d7e1cd-2f7b-4741-b3e6-f5b82d37ff92",
   "metadata": {},
   "source": [
    "<h2>Sample Problem</h2>\n",
    "We will define a set of potential materials for the insulators and the thermal materials, each with the related\n",
    "conductive property and cost.  Then for each layer,\n",
    "we will specify which materials can be used.  We will also specify the topology of the materials on the layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111c050-26f3-45f2-b71b-136a5ef9122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def make_sample_problem():\n",
    "    \"\"\"\n",
    "    Create a semiconductor-package RL problem with\n",
    "    explicit -1 cells for insulating regions.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"material_library\": {\n",
    "            # Insulators\n",
    "            \"d1\": {\"k\": 0.2,  \"cost\": 1.0},\n",
    "            \"d2\": {\"k\": 0.05, \"cost\": 0.6},\n",
    "            \"d3\": {\"k\": 0.15, \"cost\": 1.2},\n",
    "            \"d4\": {\"k\": 0.03, \"cost\": 0.5},\n",
    "            # Conductors\n",
    "            \"c1\": {\"k\": 200.0, \"cost\": 20.0},\n",
    "            \"c2\": {\"k\": 50.0,  \"cost\": 8.0},\n",
    "            \"c3\": {\"k\": 10.0,  \"cost\": 2.0},\n",
    "            \"c4\": {\"k\": 5.0,   \"cost\": 1.0},\n",
    "            \"c5\": {\"k\": 100.0, \"cost\": 12.0},\n",
    "        },\n",
    "\n",
    "        \"layers\": [\n",
    "            {\n",
    "                \"thickness\": 0.5,\n",
    "                \"insulating_type\": \"I1\",\n",
    "                \"insulating_options\": [\"d2\", \"d4\"],\n",
    "                \"thermal_regions\": [\"T1\", \"T2\"],\n",
    "                \"thermal_options\": {\n",
    "                    \"T1\": [\"c1\", \"c2\", \"c3\"],\n",
    "                    \"T2\": [\"c2\", \"c4\"],\n",
    "                },\n",
    "                # -1 marks insulating separator cells\n",
    "                \"pattern\": np.array([\n",
    "                    [-1, -1, -1, -1],\n",
    "                    [-1,  0,  0, -1],\n",
    "                    [-1,  1,  1, -1],\n",
    "                    [-1, -1, -1, -1],\n",
    "                ], dtype=np.int8),\n",
    "            },\n",
    "            {\n",
    "                \"thickness\": 0.3,\n",
    "                \"insulating_type\": \"I2\",\n",
    "                \"insulating_options\": [\"d1\", \"d2\"],\n",
    "                \"thermal_regions\": [\"T2\"],\n",
    "                \"thermal_options\": {\n",
    "                    \"T2\": [\"c2\", \"c4\"],\n",
    "                },\n",
    "                \"pattern\": np.array([\n",
    "                    [-1, -1, -1, -1],\n",
    "                    [-1,  0,  0, -1],\n",
    "                    [-1,  0,  0, -1],\n",
    "                    [-1, -1, -1, -1],\n",
    "                ], dtype=np.int8),\n",
    "            },\n",
    "            {\n",
    "                \"thickness\": 0.2,\n",
    "                \"insulating_type\": \"I3\",\n",
    "                \"insulating_options\": [\"d1\", \"d3\", \"d4\"],\n",
    "                \"thermal_regions\": [\"T1\", \"T3\"],\n",
    "                \"thermal_options\": {\n",
    "                    \"T1\": [\"c1\", \"c2\", \"c3\"],\n",
    "                    \"T3\": [\"c1\", \"c3\", \"c5\"],\n",
    "                },\n",
    "                \"pattern\": np.array([\n",
    "                    [-1,  0, -1,  1],\n",
    "                    [-1,  0, -1,  1],\n",
    "                    [-1,  0, -1,  1],\n",
    "                    [-1,  0, -1,  1],\n",
    "                ], dtype=np.int8),\n",
    "            },\n",
    "        ],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1c117a-5cc8-4e18-a36d-2b837e93bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_sample_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c39162d-faf1-4f6d-a4a6-6b25f833c393",
   "metadata": {},
   "source": [
    "<h2>The Environment</h2>\n",
    "Assumptions about the dynamics which will be used to build the simulation environment in gym.  \n",
    "\n",
    "The objective function should weight the output against the cost.\n",
    "\n",
    "This is where the core logic really lives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499222b2-046e-45a2-9c4f-102e81d7d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# Gym environment for this task\n",
    "class SemiconductorEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": []}\n",
    "\n",
    "    # Initialization function\n",
    "    def __init__(self, problem, obs_length=10):\n",
    "        super().__init__()\n",
    "        self.problem = problem\n",
    "\n",
    "        # The layers\n",
    "        self.layers = problem[\"layers\"]\n",
    "\n",
    "        # The types of materials for the insulation and thermal components which can be selected and their properties\n",
    "        self.material_library = problem[\"material_library\"]\n",
    "\n",
    "        # The number of layers\n",
    "        self.n_layers = len(self.layers)\n",
    "\n",
    "        # Start at layer 0 (the bottom)\n",
    "        self.current_layer = 0\n",
    "        self.selected_materials = []\n",
    "\n",
    "        \n",
    "        self.obs_length = obs_length\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0.0, high=1000.0, shape=(self.obs_length,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Max-size action space covering all layers\n",
    "        max_sizes = []\n",
    "        for layer in self.layers:\n",
    "            for r in layer[\"thermal_regions\"]:\n",
    "                max_sizes.append(len(layer[\"thermal_options\"][r]))\n",
    "            max_sizes.append(len(layer[\"insulating_options\"]))\n",
    "\n",
    "        # The action space is defined as a multi discrete array.  ie. a value of each insulation and thermal component. \n",
    "        self.action_space = spaces.MultiDiscrete(max_sizes)\n",
    "\n",
    "    # Reset the problem\n",
    "    def reset(self, seed=None, options=None):\n",
    "\n",
    "        # Back to layer 0\n",
    "        self.current_layer = 0\n",
    "        self.selected_materials = []\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    # The observation is the information about the next layer we need to configure.  \n",
    "    def _get_obs(self):\n",
    "        layer = self.layers[self.current_layer]\n",
    "        obs = [layer[\"thickness\"]] + \\\n",
    "              [len(layer[\"thermal_options\"][r]) for r in layer[\"thermal_regions\"]] + \\\n",
    "              [len(layer[\"insulating_options\"])]\n",
    "        obs += [0] * (self.obs_length - len(obs))  # pad\n",
    "        return np.array(obs, dtype=np.float32)\n",
    "\n",
    "\n",
    "    # Treat these problem as a multistep learning problem.  Each layer, starting at the bottom, is a step.\n",
    "    # Assign values at that layer, then step to the next layer.\n",
    "\n",
    "    # This fundamentally defines the problem and the structure of the learning.\n",
    "    def step(self, action):\n",
    "        # Flatten if VecEnv sends 2D array\n",
    "        if isinstance(action, np.ndarray) and action.ndim > 1:\n",
    "            action = action.flatten()\n",
    "    \n",
    "        layer = self.layers[self.current_layer]\n",
    "        chosen = {}\n",
    "    \n",
    "        num_regions = len(layer[\"thermal_regions\"])\n",
    "        num_options = num_regions + 1  # thermal + insulating\n",
    "    \n",
    "        # Take only first num_options entries\n",
    "        action = action[:num_options]\n",
    "    \n",
    "        # Clip each action to layer-specific max index\n",
    "        for idx, r in enumerate(layer[\"thermal_regions\"]):\n",
    "            max_idx = len(layer[\"thermal_options\"][r]) - 1\n",
    "            clipped = int(np.clip(action[idx], 0, max_idx))\n",
    "            chosen[r] = layer[\"thermal_options\"][r][clipped]\n",
    "    \n",
    "        # Insulating\n",
    "        max_idx = len(layer[\"insulating_options\"]) - 1\n",
    "        clipped = int(np.clip(action[num_regions], 0, max_idx))\n",
    "        chosen[layer[\"insulating_type\"]] = layer[\"insulating_options\"][clipped]\n",
    "\n",
    "        # Set the configuration of the current layer, and move the index to the next layer for the next step.\n",
    "        self.selected_materials.append(chosen)\n",
    "        self.current_layer += 1\n",
    "\n",
    "        # Return done if we are at the last layer\n",
    "        done = self.current_layer >= self.n_layers\n",
    "\n",
    "        # Only compute the reward if this is the last layer\n",
    "        reward = self._compute_reward() if done else 0.0\n",
    "        obs = self._get_obs() if not done else np.zeros(self.obs_length, dtype=np.float32)\n",
    "        return obs, reward, done, False, {}\n",
    "\n",
    "    # Goal is to maximize thermal conductivity in z-direction, while minimizing the cost of the materials used\n",
    "    \n",
    "    def _compute_reward(self):\n",
    "        # The total k\n",
    "        total_k_inv = 0.0\n",
    "\n",
    "        # Total cost given the materials\n",
    "        total_cost = 0.0\n",
    "\n",
    "        # For each layer, calculate \n",
    "        for layer, chosen in zip(self.layers, self.selected_materials):\n",
    "            k_sum = 0.0\n",
    "\n",
    "            # Sum up the cost and thermal impact from the thermal regions\n",
    "            for r in layer['thermal_regions']:\n",
    "                mat = chosen[r]\n",
    "                k_sum += self.material_library[mat]['k']\n",
    "                total_cost += self.material_library[mat]['cost']\n",
    "\n",
    "            # cost and thermal impact of the insulation\n",
    "            ins_mat = chosen[layer['insulating_type']]\n",
    "            k_sum += self.material_library[ins_mat]['k']\n",
    "            total_cost += self.material_library[ins_mat]['cost']\n",
    "\n",
    "            # Account for the thickness of the layer\n",
    "            k_eff = k_sum / (len(layer['thermal_regions']) + 1)\n",
    "            total_k_inv += layer['thickness'] / k_eff\n",
    "\n",
    "        K_total = 1.0 / total_k_inv\n",
    "        reward = K_total - 0.1 * total_cost\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b7095f-8d18-4d3c-a252-34c12a89f679",
   "metadata": {},
   "source": [
    "<h2>Training Loop</h2>\n",
    "\n",
    "Given an environment, the training loop is relatively straightforward.  We will use PPO to conduct the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b66a9d0-55f6-43a7-bb0f-ef9e66e8cdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import DummyVecEnv\n",
    "\n",
    "problem = make_sample_problem()\n",
    "env = DummyVecEnv([lambda: SemiconductorEnv(problem)])  \n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=5000)\n",
    "\n",
    "# Access the raw environment\n",
    "raw_env = env.envs[0]  # \n",
    "obs = raw_env.reset()[0]  # Gymnasium-style reset returns (obs, info)\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _, _ = raw_env.step(action)  # Gymnasium API\n",
    "\n",
    "\n",
    "print(\"Selected materials per layer:\", env.envs[0].selected_materials)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc007442-01a1-4472-9bca-79c53c911f70",
   "metadata": {},
   "source": [
    "<h2>Next Steps</h2>\n",
    "\n",
    "- A preference could be added to each of the materials.  This could result in more of an interactive component to the RL system. \n",
    "Solutions could be presented to the customer to rank, and the preference would be learned (RLHF-style)\n",
    "\n",
    "- The current environmental model does not consider the actual location of the materials on the layer, or the interactions between the layers.  This could be added to the SemiconductorEnv to get a more realistic behavior of the system.\n",
    "\n",
    "- Run experiments on Single Step RL (configure all layers as single action) and Multistep w/ Delayed Reward to compare the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d6360-a088-4da9-b202-690bddf81e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
