{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ac9a22-555a-4ab5-bd88-886fc416c8a5",
   "metadata": {},
   "source": [
    "<h2>What’s the Problem? </h2>\n",
    "\n",
    "Semiconductor packages use a variety of insulating and conducting materials that are patterned in a layered structure. Those materials have different thermal conductivities and depending on their pattern within each layer, they will contribute differently to the overall effective thermal conductivity of the package. \n",
    "\n",
    "A customer is interested in choosing the optimal combination of materials per layer that maximizes the thermal conductivity in the “z” direction (shown in the figure below), and minimize the materials cost subject to constraints on the available choices of material. \n",
    "\n",
    "<b>The Ask </b>\n",
    "\n",
    "Train an RL agent from scratch using Ray-RLLib that chooses the “optimal” set of materials per layer on a semiconductor package layout given information about the cost per unit weight and available choices of materials per layer per example, and details about the spatial pattern of conducting and and insulating materials in each layer. \n",
    "\n",
    "<b>What We’ll Look for</b>\n",
    "\n",
    "- Resourcefulness - were you able to come up with a self-consistent toy environment on which to train your RL agent? How did you inform your environment design by pulling relevant domain expertise?\n",
    "- Creativity - this problem is intentionally pretty open but quite technical, which mirrors most problems at Vinci. Did you come up with a novel way of solving it? Where were you blocked and how did you get around those blockers?\n",
    "- Communication - How clearly can you summarize your approach, the solution, and its implicit and chosen shortcomings?\n",
    "\n",
    "<b>Additional Details</b>\n",
    "\n",
    "You will need to construct a custom RL environment that matches the problem statement. We don’t expect you to write an RL algorithm from scratch. Feel free to use out-of box algorithm(s) from RLLib You should think about how the optimal actions of the RL agent would be consumed downstream - what would serving this model in a product look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67e661b-5034-4089-84ca-c0503118a293",
   "metadata": {},
   "source": [
    "<h2> Approach </h2>\n",
    "- We will consider three approaches to using RL to handle this problem.\n",
    "\n",
    "- Single Step (Finite horizon of 1) - Each step of RL, the \"action\" is to perturb one (or potentially more) parameters, and run the evaluation to find the reward.  The reward is assumed to be determinitistic.  ie. each action is an operation over the entire stack.  Setting the <b>entire</b> configuration of all the layers is considered a single action.\n",
    "\n",
    "- Multistep (Horizon == Number of layers) - Each timestep, the action is to set the configuration of the next layer.  Layer 1 is set at t=1, layer 2 is set at t=2, given the configuration of layer 1, layer 3 is set at t=3 given the configurations of layers 1 and 2, etc...  Under this approach, reward can be calculated two ways:\n",
    "\n",
    "    - Delayed reward.  The reward is 0 at each layer, until after the final configuration is determined.  Then the overall reward is calculated.  This approach tends to converge slower as reward after the last step have to be propogated back to determine the values of the decisions of the initial layers (ie.  How did the first layer have impact on the final outcome).  This is known as the credit assignment problem. This is analogous to using RL to solve a maze, but the only reward is given at the very end when the goal is reaching.  This then requires propogating the information back to the initial steps to figure out the best actions to take.\n",
    "  \n",
    "    - Incremental reward - Calculate a reward at each layer based on some heuristic, for example, looking at the cost and performance of the materials of the layer you just calculated.  The heuristic typically requires some domain knowledge, as you are essentially already suggesting a path.  (The heuristic ends up acting like the value function).  This information can help lead to a result faster, but risks missing the best solution if the heuristic is poor, potentially getting stuck in local minima, or taking longer to find the optimal solution.  Using the maze analogy, this would be applying a reward at each step based on the decrease in Manhattan distance to goal.  By encouraging actions which get closer to the goal in the short term, a solution might be found faster, but makes it difficult to find paths which have to move further away first but are better overall.  \n",
    "\n",
    "- RL is used in this problem as an optimization.  The goal is to explore the space of configurations to find the best configuration given the objective function.  This is different than a typical training approach which searchs model parameter space to find the best set for performance over a large training set.\n",
    "\n",
    "- The exploration conducted via RL algorithms (trying random actions) will be used to explore the configuration space.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cf4617-0bd3-474d-9e4e-ca3b8be12f9e",
   "metadata": {},
   "source": [
    "For this implementation, we will use a delayed reward implementation.  The reward is only calculated and returned after the final layer of the environment is calculated.\n",
    "\n",
    "<h3>Action Space</h3>\n",
    "\n",
    "The action sapce is a 1-d array of integers where each position represents a configuration for a material.  For each step, only the values for the layers till the current layer are set (with the remainder of the set to 0's to maintain constant array length). \n",
    "\n",
    "action = [<i>layer0_config, layer1_config, layer2_config</i>]\n",
    "<pre>\n",
    "# Used by the RL library to select the actions\n",
    "max_sizes = [3,2,2,    # layer 0 (T1, T2, I1)\n",
    "             2,2,      # layer 1 (T2, I2)\n",
    "             3,3,3]    # layer 2 (T1, T3, I3)\n",
    "\n",
    "example_action = [1,0,0,1,0,2,1,0]\n",
    "</pre>\n",
    "\n",
    "\n",
    "<h3>Observation Space</h3>\n",
    "\n",
    "The observation space includes information about the current layer which is about to be set.  This includes:\n",
    "- The layer thickness\n",
    "- The number thermal material options per material\n",
    "- The number of insulating material options\n",
    "- The pattern of the layer\n",
    "\n",
    "The observation at any time step is only for the layer we are about to configure.\n",
    "\n",
    "<pre>\n",
    "    np.array([\n",
    "    0.5,   # thickness\n",
    "    3.0,   # number of options for T1\n",
    "    2.0,   # number of options for T2\n",
    "    2.0,   # number of insulating options\n",
    "    -1., -1., -1., -1.,   # Pattern for that layer\n",
    "    -1.,  0.,  0., -1.,\n",
    "    -1.,  1.,  1., -1.,\n",
    "    -1., -1., -1., -1.\n",
    "], dtype=np.float32)\n",
    "</pre>\n",
    "\n",
    "(Note:  Some of these variables are included to simplify operations in the environment. ie. The number of options will be used as a mask when looking at the action array.   As each layer is treated independently, most of the observation space could be calculated on-demand from the problem specification).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d7e1cd-2f7b-4741-b3e6-f5b82d37ff92",
   "metadata": {},
   "source": [
    "<h2>Sample Problem</h2>\n",
    "We will define a set of potential materials for the insulators and the thermal materials, each with the related\n",
    "conductive property and cost.  Then for each layer,\n",
    "we will specify which materials can be used.  We will also specify the pattern of the materials on the layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2788f057-5b55-4975-a1d8-322874965110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def make_sample_problem():\n",
    "    \"\"\"\n",
    "    Create a semiconductor-package RL problem with\n",
    "    explicit -1 cells for insulating regions.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"material_library\": {\n",
    "            # Insulators\n",
    "            \"d1\": {\"k\": 0.2,  \"cost\": 1.0},\n",
    "            \"d2\": {\"k\": 0.05, \"cost\": 0.6},\n",
    "            \"d3\": {\"k\": 0.15, \"cost\": 1.2},\n",
    "            \"d4\": {\"k\": 0.03, \"cost\": 0.5},\n",
    "            # Conductors\n",
    "            \"c1\": {\"k\": 200.0, \"cost\": 20.0},\n",
    "            \"c2\": {\"k\": 50.0,  \"cost\": 8.0},\n",
    "            \"c3\": {\"k\": 10.0,  \"cost\": 2.0},\n",
    "            \"c4\": {\"k\": 5.0,   \"cost\": 1.0},\n",
    "            \"c5\": {\"k\": 100.0, \"cost\": 12.0},\n",
    "        },\n",
    "\n",
    "        \"layers\": [\n",
    "            {\n",
    "                \"thickness\": 0.5,\n",
    "                \"insulating_type\": \"I1\",\n",
    "                \"insulating_options\": [\"d2\", \"d4\"],\n",
    "                \"thermal_regions\": [\"T1\", \"T2\"],\n",
    "                \"thermal_options\": {\n",
    "                    \"T1\": [\"c1\", \"c2\", \"c3\"],\n",
    "                    \"T2\": [\"c2\", \"c4\"],\n",
    "                },\n",
    "                # -1 marks insulating separator cells\n",
    "                \"pattern\": np.array([\n",
    "                    [-1, -1, -1, -1],\n",
    "                    [-1,  0,  0, -1],\n",
    "                    [-1,  1,  1, -1],\n",
    "                    [-1, -1, -1, -1],\n",
    "                ], dtype=np.int8),\n",
    "            },\n",
    "            {\n",
    "                \"thickness\": 0.3,\n",
    "                \"insulating_type\": \"I2\",\n",
    "                \"insulating_options\": [\"d1\", \"d2\"],\n",
    "                \"thermal_regions\": [\"T2\"],\n",
    "                \"thermal_options\": {\n",
    "                    \"T2\": [\"c2\", \"c4\"],\n",
    "                },\n",
    "                \"pattern\": np.array([\n",
    "                    [-1, -1, -1, -1],\n",
    "                    [-1,  0,  0, -1],\n",
    "                    [-1,  0,  0, -1],\n",
    "                    [-1, -1, -1, -1],\n",
    "                ], dtype=np.int8),\n",
    "            },\n",
    "            {\n",
    "                \"thickness\": 0.2,\n",
    "                \"insulating_type\": \"I3\",\n",
    "                \"insulating_options\": [\"d1\", \"d3\", \"d4\"],\n",
    "                \"thermal_regions\": [\"T1\", \"T3\"],\n",
    "                \"thermal_options\": {\n",
    "                    \"T1\": [\"c1\", \"c2\", \"c3\"],\n",
    "                    \"T3\": [\"c1\", \"c3\", \"c5\"],\n",
    "                },\n",
    "                \"pattern\": np.array([\n",
    "                    [-1,  0, -1,  1],\n",
    "                    [-1,  0, -1,  1],\n",
    "                    [-1,  0, -1,  1],\n",
    "                    [-1,  0, -1,  1],\n",
    "                ], dtype=np.int8),\n",
    "            },\n",
    "        ],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc1c117a-5cc8-4e18-a36d-2b837e93bf3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'material_library': {'d1': {'k': 0.2, 'cost': 1.0},\n",
       "  'd2': {'k': 0.05, 'cost': 0.6},\n",
       "  'd3': {'k': 0.15, 'cost': 1.2},\n",
       "  'd4': {'k': 0.03, 'cost': 0.5},\n",
       "  'c1': {'k': 200.0, 'cost': 20.0},\n",
       "  'c2': {'k': 50.0, 'cost': 8.0},\n",
       "  'c3': {'k': 10.0, 'cost': 2.0},\n",
       "  'c4': {'k': 5.0, 'cost': 1.0},\n",
       "  'c5': {'k': 100.0, 'cost': 12.0}},\n",
       " 'layers': [{'thickness': 0.5,\n",
       "   'insulating_type': 'I1',\n",
       "   'insulating_options': ['d2', 'd4'],\n",
       "   'thermal_regions': ['T1', 'T2'],\n",
       "   'thermal_options': {'T1': ['c1', 'c2', 'c3'], 'T2': ['c2', 'c4']},\n",
       "   'pattern': array([[-1, -1, -1, -1],\n",
       "          [-1,  0,  0, -1],\n",
       "          [-1,  1,  1, -1],\n",
       "          [-1, -1, -1, -1]], dtype=int8)},\n",
       "  {'thickness': 0.3,\n",
       "   'insulating_type': 'I2',\n",
       "   'insulating_options': ['d1', 'd2'],\n",
       "   'thermal_regions': ['T2'],\n",
       "   'thermal_options': {'T2': ['c2', 'c4']},\n",
       "   'pattern': array([[-1, -1, -1, -1],\n",
       "          [-1,  0,  0, -1],\n",
       "          [-1,  0,  0, -1],\n",
       "          [-1, -1, -1, -1]], dtype=int8)},\n",
       "  {'thickness': 0.2,\n",
       "   'insulating_type': 'I3',\n",
       "   'insulating_options': ['d1', 'd3', 'd4'],\n",
       "   'thermal_regions': ['T1', 'T3'],\n",
       "   'thermal_options': {'T1': ['c1', 'c2', 'c3'], 'T3': ['c1', 'c3', 'c5']},\n",
       "   'pattern': array([[-1,  0, -1,  1],\n",
       "          [-1,  0, -1,  1],\n",
       "          [-1,  0, -1,  1],\n",
       "          [-1,  0, -1,  1]], dtype=int8)}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_sample_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c39162d-faf1-4f6d-a4a6-6b25f833c393",
   "metadata": {},
   "source": [
    "<h2>The Environment</h2>\n",
    "Assumptions about the dynamics which will be used to build the simulation environment in gym.  \n",
    "\n",
    "The objective function should weight the output against the cost.\n",
    "\n",
    "This is where the core logic really lives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5962425a-6164-493e-8603-9901f0c8a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class SemiconductorEnvV2(gym.Env):\n",
    "    metadata = {\"render_modes\": []}\n",
    "\n",
    "    def __init__(self, problem, obs_length=10):\n",
    "        super().__init__()\n",
    "        self.problem = problem\n",
    "\n",
    "        # Copy th elayers from the problem\n",
    "        self.layers = problem[\"layers\"]\n",
    "\n",
    "        # Get the materials library\n",
    "        self.material_library = problem[\"material_library\"]\n",
    "\n",
    "        # Get the number of layers\n",
    "        self.n_layers = len(self.layers)\n",
    "\n",
    "        # Determine pattern size (assume all layers share the same grid size)\n",
    "        self.pattern_shape = self.layers[0][\"pattern\"].shape\n",
    "        self.pattern_len   = self.pattern_shape[0] * self.pattern_shape[1]\n",
    "\n",
    "        # observation: info about the materials + flattened pattern\n",
    "        # We'll override obs_length to guarantee enough room\n",
    "        self.obs_length = 1 + 1 + max(len(l[\"thermal_regions\"]) for l in self.layers) \\\n",
    "                            + self.pattern_len\n",
    "        # 1 thickness, up to N thermal option counts, 1 insulating count, pattern_len\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0.0, high=1000.0, shape=(self.obs_length,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Max-size action space covering all layers\n",
    "        # This is the maximum value of each entry.  How many different materials can this component select from?\n",
    "        max_sizes = []\n",
    "        for layer in self.layers:\n",
    "            for r in layer[\"thermal_regions\"]:\n",
    "                max_sizes.append(len(layer[\"thermal_options\"][r]))\n",
    "            max_sizes.append(len(layer[\"insulating_options\"]))\n",
    "\n",
    "        # The action space is defined as a multi discrete array.  ie. a value of assigned each insulation and thermal component.\n",
    "        # This will be used by the training algorithms to select actions as part of exploration/learning\n",
    "        self.action_space = spaces.MultiDiscrete(max_sizes)\n",
    "\n",
    "        # Initialize the current layer and the current selected materials\n",
    "        self.current_layer = 0\n",
    "        self.selected_materials = []\n",
    "\n",
    "    # Reset the problem\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_layer = 0\n",
    "        self.selected_materials = []\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    # Get the observations.  This is really just the state of the world.  (Not doing parital observability here)\n",
    "    # The observation is the information about the next layer we need to configure.  \n",
    "    # This only describes the layer we are about to configure.  \n",
    "    # Values in the observation vector include:\n",
    "    # - Thickness\n",
    "    # - Number of values each material can take (used for filtering out the action vector)\n",
    "    # - The pattern of materials on the layer. Flattened.\n",
    "    def _get_obs(self):\n",
    "        layer = self.layers[self.current_layer]\n",
    "\n",
    "        # Basic numeric info\n",
    "        obs = [layer[\"thickness\"]]\n",
    "        for r in layer[\"thermal_regions\"]:\n",
    "            obs.append(len(layer[\"thermal_options\"][r]))\n",
    "        obs.append(len(layer[\"insulating_options\"]))\n",
    "\n",
    "        # Flatten pattern and normalize to [0,1] (convert -1 to 0 for padding/insulator indicator)\n",
    "        pat = layer[\"pattern\"].astype(np.float32)\n",
    "        pat = np.where(pat < 0, 0, pat)  # -1 -> 0\n",
    "        pat = pat / (pat.max() if pat.max() > 0 else 1.0)\n",
    "        obs.extend(pat.flatten())\n",
    "\n",
    "        # Zero-pad to full length if needed\n",
    "        if len(obs) < self.obs_length:\n",
    "            obs += [0.0] * (self.obs_length - len(obs))\n",
    "        return np.array(obs, dtype=np.float32)\n",
    "\n",
    "    # Define the step function describing how the environment operatoes. \n",
    "    # Treat these problem as a multistep learning problem.  Each layer, starting at the bottom, is a step.\n",
    "    # Assign values at that layer, then step to the next layer.\n",
    "    # This fundamentally defines the problem and the structure of the learning. \n",
    "    def step(self, action):\n",
    "        if isinstance(action, np.ndarray) and action.ndim > 1:\n",
    "            action = action.flatten()\n",
    "\n",
    "        layer = self.layers[self.current_layer]\n",
    "        chosen = {}\n",
    "        num_regions = len(layer[\"thermal_regions\"])\n",
    "        num_options = num_regions + 1\n",
    "\n",
    "        # Take only first num_options entries. ie. only consider the part of the action space related to the layers so far.\n",
    "        # Ignore the rest of the vector as those consider actions related to future layers.\n",
    "        action = action[:num_options]\n",
    "\n",
    "        # Choose thermal materials\n",
    "        for idx, r in enumerate(layer[\"thermal_regions\"]):\n",
    "            max_idx = len(layer[\"thermal_options\"][r]) - 1\n",
    "            chosen[r] = layer[\"thermal_options\"][r][int(np.clip(action[idx], 0, max_idx))]\n",
    "\n",
    "        # Choose insulating material\n",
    "        max_idx = len(layer[\"insulating_options\"]) - 1\n",
    "        chosen[layer[\"insulating_type\"]] = layer[\"insulating_options\"][int(np.clip(action[num_regions], 0, max_idx))]\n",
    "\n",
    "        self.selected_materials.append(chosen)\n",
    "        self.current_layer += 1\n",
    "\n",
    "        # Determine if we have reached the last layer\n",
    "        done = self.current_layer >= self.n_layers\n",
    "\n",
    "        # If so, calculate the reward. Otherwise, reward is just 0.  This is the delayed reward approach\n",
    "        reward = self._compute_reward() if done else 0.0\n",
    "        obs = self._get_obs() if not done else np.zeros(self.obs_length, dtype=np.float32)\n",
    "        return obs, reward, done, False, {}\n",
    "\n",
    "    \n",
    "    # Goal is to maximize thermal conductivity in z-direction, while minimizing the cost of the materials used\n",
    "    def _compute_reward(self):\n",
    "\n",
    "        # The total k\n",
    "        total_k_inv = 0.0\n",
    "\n",
    "        # The total cost\n",
    "        total_cost = 0.0\n",
    "\n",
    "        # For each layer, calculate the reward of that layer\n",
    "        for layer, chosen in zip(self.layers, self.selected_materials):\n",
    "            pattern = layer[\"pattern\"]\n",
    "            total_cells = np.sum(pattern >= 0)  # ignore -1 insulating separators\n",
    "\n",
    "            # accumulate weighted conductivity and cost\n",
    "            layer_k = 0.0\n",
    "            layer_cost = 0.0\n",
    "\n",
    "            \n",
    "            # Sum up the cost and thermal impact from the thermal regions, weighted by the percentage of the layout they take up          \n",
    "            for r_idx, region in enumerate(layer[\"thermal_regions\"]):\n",
    "                region_cells = np.sum(pattern == r_idx)\n",
    "                frac = region_cells / total_cells if total_cells > 0 else 0\n",
    "                mat = chosen[region]\n",
    "                layer_k += frac * self.material_library[mat][\"k\"]\n",
    "                layer_cost += frac * self.material_library[mat][\"cost\"]\n",
    "\n",
    "            # Same thing for the insulating material weight\n",
    "            ins_cells = np.sum(pattern == -1)\n",
    "            frac_ins = ins_cells / (pattern.size)  # entire grid fraction\n",
    "            ins_mat = chosen[layer[\"insulating_type\"]]\n",
    "            layer_k += frac_ins * self.material_library[ins_mat][\"k\"]\n",
    "            layer_cost += frac_ins * self.material_library[ins_mat][\"cost\"]\n",
    "\n",
    "            # Account for the thickness of the layer\n",
    "            k_eff = layer_k\n",
    "            total_k_inv += layer['thickness'] / k_eff if k_eff > 0 else np.inf\n",
    "            total_cost += layer_cost\n",
    "\n",
    "        K_total = 1.0 / total_k_inv if total_k_inv > 0 else 0.0\n",
    "        return K_total - 0.1 * total_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b7095f-8d18-4d3c-a252-34c12a89f679",
   "metadata": {},
   "source": [
    "<h2>Training Loop</h2>\n",
    "\n",
    "Given an environment, the training loop is relatively straightforward.  We will use PPO to conduct the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7edf9ce3-a36e-46cf-a38d-b75b93c685f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aatrash/deeplearning/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 630  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 551        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02599395 |\n",
      "|    clip_fraction        | 0.425      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -7.14      |\n",
      "|    explained_variance   | 0.000477   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 161        |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0617    |\n",
      "|    value_loss           | 610        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 536         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021317309 |\n",
      "|    clip_fraction        | 0.481       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.08       |\n",
      "|    explained_variance   | -4.03e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 279         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0686     |\n",
      "|    value_loss           | 719         |\n",
      "-----------------------------------------\n",
      "Selected materials per layer: [{'T1': 'c1', 'T2': 'c2', 'I1': 'd4'}, {'T2': 'c2', 'I2': 'd2'}, {'T1': 'c1', 'T3': 'c1', 'I3': 'd1'}]\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import DummyVecEnv\n",
    "\n",
    "problem = make_sample_problem()\n",
    "env = DummyVecEnv([lambda: SemiconductorEnvV2(problem)])  # uses new class\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=5000)\n",
    "\n",
    "raw_env = env.envs[0]\n",
    "obs = raw_env.reset()[0]\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _, _ = raw_env.step(action)\n",
    "\n",
    "print(\"Selected materials per layer:\", raw_env.selected_materials)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc007442-01a1-4472-9bca-79c53c911f70",
   "metadata": {},
   "source": [
    "<h2>Next Steps</h2>\n",
    "\n",
    "- A preference could be added to each of the materials.  There may be more involved than the cost.  This could result in more of an interactive component to the RL system. Solutions could be presented to the customer to rank, and the preference would be learned (RLHF-style)\n",
    "\n",
    "- The current environmental model does not consider the interactions between the layers.  This could be added to the SemiconductorEnv to get a more realistic behavior of the system.\n",
    "\n",
    "- Experiments should be running using Single Step RL (configure all layers as single action) and Multistep w/ Incremental Reward to compare the performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
